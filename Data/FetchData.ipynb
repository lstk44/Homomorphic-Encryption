{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# MedMNIST\n",
    "import medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Dectection Data\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "The data used is provided by Loghub, which maintains a collection of system logs that are freely accessible for AI-driven log analytics research . The logs are a combination of production data released from previous studies and real systems in their lab environment. The logs are not sanitized, anonymized, or modified in any way, wherever possible. These log datasets are freely available for research or academic work.\n",
    "\n",
    "https://github.com/logpai/loghub/tree/master\n",
    "\n",
    "**Dataset  1:**\n",
    "\n",
    "_Android_\n",
    "\n",
    "Loghub Description:\n",
    "\n",
    "Android (https://www.android.com) is a popular open-source mobile operating system and has been used by many smart devices. However, Android logs are rarely available in public for research purposes. We provide some Android log files, which were collected by Android smartphones with heavily instrumented modules installed. The Android architecture comprises of five levels, including the Linux Kernel, Libraries, Application Framework, Android Runtime, and System Applications. We provide a sample log file printed by the Application Framework.\n",
    "\n",
    "Training on logs: [Info (I), Debug (D), Verbose (V)]\n",
    "\n",
    "Test detecting: [Warn (W), Error (E)]\n",
    "\n",
    "https://github.com/logpai/loghub/blob/master/Android/Android_2k.log_structured.csv\n",
    "\n",
    "**Dataset 2:**\n",
    "\n",
    "_BGL_\n",
    "\n",
    "Loghub Description:\n",
    "\n",
    "BGL is an open dataset of logs collected from a BlueGene/L supercomputer system at Lawrence Livermore National Labs (LLNL) in Livermore, California, with 131,072 processors and 32,768GB memory. The log contains alert and non-alert messages identified by alert category tags. In the first column of the log, \"-\" indicates non-alert messages while others are alert messages. The label information is amenable to alert detection and prediction research. It has been used in several studies on log parsing, anomaly detection, and failure prediction.\n",
    "\n",
    "Training on logs: [Info, Warning]\n",
    "\n",
    "Test detecting: [Error, Fatal, Severe]\n",
    "\n",
    "https://github.com/logpai/loghub/blob/master/BGL/BGL_2k.log_structured.csv\n",
    "\n",
    "**Dataset 3:**\n",
    "\n",
    "_Hadoop_\n",
    "\n",
    "Loghub Description:\n",
    "\n",
    "Hadoop is a big data processing framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.The logs are generated from a Hadoop cluster with 46 cores across five machines simulating both normal and abnormal cases with injected specific failures for two applications (WordCount & PageRank)\n",
    "\n",
    "Training on logs: [Info, Warn]\n",
    "\n",
    "Test detecting: [Error, Fatal]\n",
    "\n",
    "https://github.com/logpai/loghub/blob/master/Hadoop/Hadoop_2k.log_structured.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_from_github(\n",
    "        url: str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Download anomaly dectection data from github\n",
    "\n",
    "    Input:\n",
    "        url: github url\n",
    "\n",
    "    Output:\n",
    "        datasets: dictionary of dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    # create temporary folder\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    # clone github repo to temporary folder\n",
    "    !git clone $url $tmp_dir\n",
    "    # store dataframes in a dictionary\n",
    "    data_dictionary = {}\n",
    "    # loop through all folders in the temporary folder and get all csv files ending with structured.csv\n",
    "    for root, dirs, files in os.walk(tmp_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('structured.csv'):\n",
    "                log_name = file.split('_')[0].lower()\n",
    "                data_dictionary[log_name] = pd.read_csv(os.path.join(root, file))\n",
    "    # delete temporary folder\n",
    "    shutil.rmtree(tmp_dir)\n",
    "    # return dictionary of dataframes\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def split_data(\n",
    "        data_dictionary: dict\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets\n",
    "\n",
    "    Input:\n",
    "        data: dictionary of dataframes\n",
    "\n",
    "    Output:\n",
    "        datasets: dictionary of train and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    data = data_dictionary.copy()\n",
    "    # dictionary of train and test sets\n",
    "    data_dictionary = {}\n",
    "    # loop through datasets\n",
    "    for d in ['android', 'bgl', 'hadoop']:\n",
    "        df = data[d]\n",
    "        # split into normal and anomaly data\n",
    "        normal_data, anomaly_data = df[df.Level.isin(['INFO', 'WARN', 'WARNING', 'I', 'D', 'V'])]['Content'], df[~df.Level.isin(['INFO', 'WARN', 'WARNING', 'I', 'D', 'V'])]['Content']\n",
    "        # test dataset is a 50/50 split of normal and anomaly data (50 observations each)\n",
    "        normal_sample, anomaly_sample = normal_data.sample(50), anomaly_data.sample(50)\n",
    "        xtest, ytest = pd.concat([normal_sample, anomaly_sample]), np.concatenate([np.zeros(50), np.ones(50)])\n",
    "        # train dataset is the remaining normal data\n",
    "        xtrain = normal_data.drop(normal_sample.index)\n",
    "        # add train and test sets to dictionary\n",
    "        data_dictionary[d] = {'xtrain': xtrain, 'xtest': xtest, 'ytest': ytest}\n",
    "    # return dictionary of train and test sets\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def data_preprocessing(\n",
    "        data_dictionary: dict,\n",
    "    ):\n",
    "    '''\n",
    "    Simple data preprocessing pipeline\n",
    "\n",
    "    Input:\n",
    "        data_dictionary: dictionary of dataframes\n",
    "\n",
    "    Output:\n",
    "        data_dictionary: dictionary of preprocessed dataframes\n",
    "    '''\n",
    "\n",
    "    regex_patterns = {\n",
    "        # CamelCase split\n",
    "        r'(?<=[a-z0-9])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])': ' ',\n",
    "        # remove non-alphanumeric characters\n",
    "        r'[^a-zA-Z]':                                       ' ',\n",
    "        # remove extra spaces\n",
    "        r'\\s+':                                             ' '\n",
    "    }\n",
    "\n",
    "    # loop through datasets xtrain and xtest\n",
    "    for d in ['android', 'bgl', 'hadoop']:\n",
    "        for t in ['xtrain', 'xtest']:\n",
    "            # preprocess data\n",
    "            data = data_dictionary[d][t]\n",
    "            # regex\n",
    "            for regex, replacement in regex_patterns.items():\n",
    "                data = data.str.replace(regex, replacement, regex=True)\n",
    "            # convert to lowercase\n",
    "            data = data.str.lower()\n",
    "            # strip leading and trailing spaces\n",
    "            data = data.str.strip()\n",
    "            # replace with preprocessed data\n",
    "            data_dictionary[d][t] = data\n",
    "    # return dictionary of preprocessed dataframes\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def tfidf_vectorizer(\n",
    "        data_dictionary: dict,\n",
    "        max_features: int = 1000\n",
    "    ):\n",
    "    '''\n",
    "    TF-IDF Vectorizer\n",
    "\n",
    "    Input:\n",
    "        data_dictionary: dictionary of dataframes\n",
    "        max_features: maximum number of features\n",
    "\n",
    "    Output:\n",
    "        data_dictionary: dictionary of dataframes with tfidf vectors\n",
    "    '''\n",
    "    # loop through datasets\n",
    "    for d in ['android', 'bgl', 'hadoop']:\n",
    "        # split into train and test sets\n",
    "        x_train, x_test = data_dictionary[d]['xtrain'], data_dictionary[d]['xtest']\n",
    "        # initialize tfidf vectorizer\n",
    "        tfidf = TfidfVectorizer(max_features=max_features)\n",
    "        # fit TfidfVectorizer\n",
    "        tfidf.fit(x_train)\n",
    "        # transform xtrain, xtest\n",
    "        xtrain_tfidf = x_train.apply(lambda x: list(tfidf.transform([x]).toarray().reshape(-1)))\n",
    "        xtest_tfidf = x_test.apply(lambda x: list(tfidf.transform([x]).toarray().reshape(-1)))\n",
    "        # replace with tfidf vectors\n",
    "        data_dictionary[d]['xtrain'], data_dictionary[d]['xtest'] = xtrain_tfidf, xtest_tfidf\n",
    "    # return dictionary of dataframes\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def save_data(\n",
    "        data_dictionary: dict\n",
    "    ):\n",
    "    '''\n",
    "    Save dataframes to numpy files\n",
    "\n",
    "    Input:\n",
    "        data_dictionary: dictionary of dataframes\n",
    "\n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # loop through datasets\n",
    "    for d in ['android', 'bgl', 'hadoop']:\n",
    "        # loop through train and test sets\n",
    "        for t in ['xtrain', 'xtest', 'ytest']:\n",
    "            # save dataframe to numpy file\n",
    "            np.save(f'{d}_{t}.npy', data_dictionary[d][t])\n",
    "    print('Data successfully saved to csv files!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tmp0zc12b4q'...\n",
      "remote: Enumerating objects: 563, done.\u001b[K\n",
      "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 563 (delta 59), reused 58 (delta 50), pack-reused 473\u001b[K\n",
      "Receiving objects: 100% (563/563), 7.32 MiB | 591.00 KiB/s, done.\n",
      "Resolving deltas: 100% (259/259), done.\n"
     ]
    }
   ],
   "source": [
    "datasets = download_data_from_github('https://github.com/logpai/loghub.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = split_data(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = data_preprocessing(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tfidf_vectorizer(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to csv files!\n"
     ]
    }
   ],
   "source": [
    "save_data(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Data\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "The data used is provided by MedMNIST v2, a comprehensive collection of standardized biomedical images. It encompasses 12 datasets for 2D and 6 for 3D images, pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with corresponding classification labels. With 708,069 2D images and 9,998 3D images, it supports various classification tasks, from binary/multi-class to ordinal regression and multi-label, catering to biomedical image analysis, computer vision, and machine learning research and education.\n",
    "\n",
    "https://medmnist.com/\n",
    "\n",
    "**Dataset  1:**\n",
    "\n",
    "_PneumoniaMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/pneumoniamnist.npz?download=1\n",
    "\n",
    "**Dataset 2:**\n",
    "\n",
    "_BreastMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/breastmnist.npz?download=1\n",
    "\n",
    "**Dataset 3:**\n",
    "\n",
    "_OrganCMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The OrganCMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Coronal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in coronal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/organcmnist.npz?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_from_medmnist(\n",
    "        dataset_list: list\n",
    "    ):\n",
    "    '''\n",
    "    Load datasets from MedMNIST\n",
    "\n",
    "    Input:\n",
    "        dataset_list: list of datasets to load\n",
    "\n",
    "    Output:\n",
    "        data_dictionary: dictionary of datasets\n",
    "    '''\n",
    "\n",
    "    data_dictionary = {}\n",
    "    # loop through datasets\n",
    "    for d in dataset_list:\n",
    "        # initialize DataClass\n",
    "        DataClass = getattr(medmnist, d)\n",
    "        # download data\n",
    "        train_dataset = DataClass(split='train',download=True)\n",
    "        eval_dataset = DataClass(split='val', download=True)\n",
    "        test_dataset = DataClass(split='test', download=True)\n",
    "        # to numpy array\n",
    "        x_train, x_val, x_test = train_dataset.imgs, eval_dataset.imgs, test_dataset.imgs\n",
    "        y_train, y_val, y_test = train_dataset.labels, eval_dataset.labels, test_dataset.labels\n",
    "\n",
    "        data_dictionary[d] = {'xtrain': x_train, 'xval': x_val, 'xtest': x_test, 'ytrain': y_train, 'yval': y_val, 'ytest': y_test}\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def split_data(\n",
    "        data_dictionary: dict\n",
    "    ):\n",
    "    '''\n",
    "    Split data into train and test sets\n",
    "\n",
    "    Input:\n",
    "        data_dictionary: dictionary of dataframes\n",
    "\n",
    "    Output:\n",
    "        data_dictionary: dictionary of train and test sets\n",
    "    '''\n",
    "\n",
    "    data = data_dictionary.copy()\n",
    "    # dictionary of train, val and test sets\n",
    "    data_dictionary = {}\n",
    "    # loop through datasets\n",
    "    for d in data.keys():\n",
    "        # concatenate X and y\n",
    "        x = np.concatenate([data[d]['xtrain'], data[d]['xval'], data[d]['xtest']], axis=0)\n",
    "        y = np.concatenate([data[d]['ytrain'], data[d]['yval'], data[d]['ytest']], axis=0)\n",
    "        # split into train, val, test=100 images\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=100, stratify=y)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)\n",
    "        # expand dimension (grayscale)\n",
    "        # (channels=1, height=28, width=28)\n",
    "        x_train = np.expand_dims(x_train, axis=1)\n",
    "        x_val = np.expand_dims(x_val, axis=1)\n",
    "        x_test = np.expand_dims(x_test, axis=1)\n",
    "        # add train, val and test sets to dictionary\n",
    "        data_dictionary[d] = {'xtrain': x_train, 'xval': x_val, 'xtest': x_test, 'ytrain': y_train, 'yval': y_val, 'ytest': y_test}\n",
    "    # return dictionary of train and test sets\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def data_preprocessing(\n",
    "        data_dictionary: dict,\n",
    "    ):\n",
    "    '''\n",
    "    Simple data preprocessing pipeline to normalize pixel values\n",
    "\n",
    "    Input:\n",
    "        data_dictionary: dictionary of dataframes\n",
    "\n",
    "    Output:\n",
    "        data_dictionary: dictionary of preprocessed dataframes\n",
    "    '''\n",
    "\n",
    "    # loop through datasets\n",
    "    for d in data_dictionary.keys():\n",
    "        # normalize pixel values\n",
    "        data_dictionary[d]['xtrain'] = data_dictionary[d]['xtrain'].astype('float32') / 255.\n",
    "        data_dictionary[d]['xval'] = data_dictionary[d]['xval'].astype('float32') / 255.\n",
    "        data_dictionary[d]['xtest'] = data_dictionary[d]['xtest'].astype('float32') / 255.\n",
    "    # return dictionary of preprocessed dataframes\n",
    "    return data_dictionary\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "def save_data(\n",
    "        data_dictionary: dict\n",
    "    ):\n",
    "    '''\n",
    "    Save dataframes to numpy files\n",
    "\n",
    "    Input:\n",
    "        data_dictionary: dictionary of dataframes\n",
    "\n",
    "    Output:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # loop through datasets\n",
    "    for d in data_dictionary.keys():\n",
    "        # loop through train and test sets\n",
    "        for t in ['xtrain', 'xval', 'xtest', 'ytrain', 'yval', 'ytest']:\n",
    "            # save dataframe to csv file\n",
    "            np.save(f'{d}_{t}.npy', data_dictionary[d][t])\n",
    "    print('Data successfully saved to csv files!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = download_data_from_medmnist(['PneumoniaMNIST', 'BreastMNIST', 'OrganCMNIST', 'DermaMNIST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = split_data(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = data_preprocessing(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to csv files!\n"
     ]
    }
   ],
   "source": [
    "save_data(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concrete",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
