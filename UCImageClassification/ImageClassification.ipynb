{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Classification\n",
    "\n",
    "**Company Use Case:**\n",
    "\n",
    "In healthcare, medical image classification is critical for accurate diagnosis but often involves handling sensitive patient data. Implementing homomorphic encryption (HE) in this context can revolutionize patient privacy and data security. By encrypting medical images, HE enables secure image processing in the cloud without revealing actual patient data. This process allows for advanced machine learning models to classify these images, assisting in diagnoses while maintaining complete data confidentiality.\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "The following Notebook will show 2 approaches to do this with ConcreteML.\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "The data used is provided by MedMNIST v2, a comprehensive collection of standardized biomedical images. It encompasses 12 datasets for 2D and 6 for 3D images, pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with corresponding classification labels. With 708,069 2D images and 9,998 3D images, it supports various classification tasks, from binary/multi-class to ordinal regression and multi-label, catering to biomedical image analysis, computer vision, and machine learning research and education.\n",
    "\n",
    "https://medmnist.com/\n",
    "\n",
    "**Dataset  1:**\n",
    "\n",
    "_PneumoniaMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/pneumoniamnist.npz?download=1\n",
    "\n",
    "**Dataset 2:**\n",
    "\n",
    "_BreastMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/breastmnist.npz?download=1\n",
    "\n",
    "**Dataset 3:**\n",
    "\n",
    "_OrganCMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The OrganCMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Coronal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in coronal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/organcmnist.npz?download=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHE Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhe_mode = 'execute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# ConcreteML\n",
    "import concrete.ml as cml\n",
    "from concrete.ml.sklearn import NeuralNetClassifier\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "\n",
    "# Skorch\n",
    "from skorch.callbacks import EpochScoring\n",
    "\n",
    "# Brevitas\n",
    "import brevitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert ConcreteML version 1.3.0\n",
    "assert cml.__version__ == '1.3.0', 'ConcreteML version 1.3.0 required'\n",
    "# # # print ConcreteML version\n",
    "print(f'ConcreteML Version: {cml.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Plot Images\n",
    "################################################################################\n",
    "\n",
    "def plot_images(x_train, y_train, dataset):\n",
    "\n",
    "    # plot sample images of each class\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    if n_classes == 2:\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "        ax = ax.flatten()\n",
    "        for i in range(n_classes):\n",
    "            ax[i].imshow(x_train[y_train.flatten()==i][0][0], cmap='gray')\n",
    "            ax[i].set_title(f'Class {i}')\n",
    "            ax[i].axis('off')\n",
    "    \n",
    "    elif n_classes > 2:\n",
    "        fig, ax = plt.subplots(nrows=3, ncols=n_classes//2, figsize=(10, 5))\n",
    "        ax = ax.flatten()\n",
    "        for i in range(n_classes):\n",
    "            ax[i].imshow(x_train[y_train.flatten()==i][0][0], cmap='gray')\n",
    "            ax[i].set_title(f'Class {i}')\n",
    "            ax[i].axis('off')\n",
    "        # remove empty subplots\n",
    "        for i in range(n_classes, len(ax)):\n",
    "            fig.delaxes(ax[i])\n",
    "\n",
    "    fig.suptitle(dataset)\n",
    "    plt.show()\n",
    "\n",
    "################################################################################\n",
    "# Plot Classification Metrics\n",
    "################################################################################\n",
    "\n",
    "def plot_classification_metrics(\n",
    "    y_true: np.array,\n",
    "    y_pred: np.array,\n",
    "    plot_title: str = None\n",
    "    ):\n",
    "\n",
    "    '''\n",
    "    Plots Classification Metrics\n",
    "\n",
    "    Input:\n",
    "      y_true = ground truth labels\n",
    "      y_pred = prediction labels\n",
    "      plot_title = title for results plot (optional)\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(5, 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        accuracy = round(accuracy_score(y_true, y_pred),2)\n",
    "        precision = round(precision_score(y_true, y_pred),2)\n",
    "        recall = round(recall_score(y_true, y_pred),2)\n",
    "        f1 = round(f1_score(y_true, y_pred),2)\n",
    "        roc_auc = round(roc_auc_score(y_true, y_pred), 2)\n",
    "        ax[0].bar(['Accuracy', 'Precision', 'Recall', 'F1'], [accuracy, precision, recall, f1])\n",
    "    else:\n",
    "        accuracy = round(accuracy_score(y_true, y_pred),2)\n",
    "        precision = round(precision_score(y_true, y_pred, average='weighted'),2)\n",
    "        recall = round(recall_score(y_true, y_pred, average='weighted'),2)\n",
    "        f1 = round(f1_score(y_true, y_pred, average='weighted'),2)\n",
    "        ax[0].bar(['Accuracy', 'Precision', 'Recall', 'F1'], [accuracy, precision, recall, f1])\n",
    "\n",
    "    # barchart of metrics for each classifier\n",
    "    ax[0].set_title('Classifier Metrics')\n",
    "    ax[0].set_ylim(0,1)\n",
    "    ax[0].bar_label(ax[0].containers[0], label_type='center')\n",
    "\n",
    "    # confusion matrix for each classifier\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax[1], cmap='Blues', colorbar=False)\n",
    "    ax[1].set_title('Classifier Confusion Matrix')\n",
    "\n",
    "    plt.suptitle(plot_title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pneumonia = np.load('../Data/PneumoniaMNIST.npz')\n",
    "xtrain_pneumonia, xval_pneumonia, xtest_pneumonia = data_pneumonia['xtrain'], data_pneumonia['xval'], data_pneumonia['xtest']\n",
    "ytrain_pneumonia, yval_pneumonia, ytest_pneumonia = data_pneumonia['ytrain'], data_pneumonia['yval'], data_pneumonia['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_breast = np.load('../Data/BreastMNIST.npz')\n",
    "xtrain_breast, xval_breast, xtest_breast = data_breast['xtrain'], data_breast['xval'], data_breast['xtest']\n",
    "ytrain_breast, yval_breast, ytest_breast = data_breast['ytrain'], data_breast['yval'], data_breast['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_organ = np.load('../Data/OrganCMNIST.npz')\n",
    "xtrain_organ, xval_organ, xtest_organ = data_organ['xtrain'], data_organ['xval'], data_organ['xtest']\n",
    "ytrain_organ, yval_organ, ytest_organ = data_organ['ytrain'], data_organ['yval'], data_organ['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(xtrain_pneumonia, ytrain_pneumonia, dataset='PneumoniaMNIST')\n",
    "plot_images(xtrain_breast, ytrain_breast, dataset='BreastMNIST')\n",
    "plot_images(xtrain_organ, ytrain_organ, dataset='OrganCMNIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnANN():\n",
    "\n",
    "    def __init__(self, epochs=50, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_layers = n_layers\n",
    "        self.neuron_factor = neuron_factor\n",
    "        self.model = None\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None,\n",
    "            'accuracy': None,\n",
    "            'precision': None,\n",
    "            'recall': None,\n",
    "            'f1': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, plot=True):\n",
    "        # reshape data\n",
    "        x_train, x_val = x_train.reshape(x_train.shape[0], -1), x_val.reshape(x_val.shape[0], -1)\n",
    "        # initialize model\n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=tuple([int(x_train.shape[1] * self.neuron_factor) for i in range(self.n_layers)]),\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=self.learning_rate,\n",
    "            alpha=self.weight_decay,\n",
    "            batch_size=128\n",
    "        )\n",
    "        # train model\n",
    "        train_l, val_l, train_score, val_score = [], [], [], []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            start_time = time.time()\n",
    "            self.model.partial_fit(x_train, y_train, classes=np.unique(y_train))\n",
    "            self.log['train'] += time.time() - start_time\n",
    "            # curve metrics\n",
    "            pred_train, pred_val = self.model.predict_proba(x_train), self.model.predict_proba(x_val)\n",
    "            train_l.append(log_loss(y_train, pred_train, labels=np.unique(y_train)))\n",
    "            val_l.append(log_loss(y_val, pred_val, labels=np.unique(y_val)))\n",
    "            train_score.append(accuracy_score(y_train, np.argmax(pred_train, axis=1)))\n",
    "            val_score.append(accuracy_score(y_val, np.argmax(pred_val, axis=1)))\n",
    "\n",
    "        # plot results\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1].set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        # reshape data for sklearn\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "        # calculate metrics\n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            self.log['accuracy'] = round(accuracy_score(y_true, y_pred), 2)\n",
    "            self.log['precision'] = round(precision_score(y_true, y_pred), 2)\n",
    "            self.log['recall'] = round(recall_score(y_true, y_pred), 2)\n",
    "            self.log['f1'] = round(f1_score(y_true, y_pred), 2)\n",
    "        else:\n",
    "            self.log['accuracy'] = round(accuracy_score(y_true, y_pred), 2)\n",
    "            self.log['precision'] = round(precision_score(y_true, y_pred, average='macro'), 2)\n",
    "            self.log['recall'] = round(recall_score(y_true, y_pred, average='macro'), 2)\n",
    "            self.log['f1'] = round(f1_score(y_true, y_pred, average='macro'), 2)\n",
    "        # plot results\n",
    "        plot_classification_metrics(y_true, y_pred, plot_title='Sklearn ANN')\n",
    "        # return prediction labels\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "sklearn_ann = SklearnANN(\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5\n",
    ")\n",
    "# train model\n",
    "sklearn_ann = sklearn_ann.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    plot=True\n",
    ")\n",
    "# evaluate model\n",
    "ypred_ann_sklearn = sklearn_ann.evaluate(xtest_pneumonia, ytest_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(sklearn_ann.log, index=[0])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "sklearn_ann = SklearnANN(\n",
    "    epochs=50,\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.5,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.3,\n",
    ")\n",
    "# train model\n",
    "sklearn_ann = sklearn_ann.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    plot=True\n",
    ")\n",
    "# evaluate model\n",
    "ypred_ann_sklearn = sklearn_ann.evaluate(xtest_breast, ytest_breast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(sklearn_ann.log, index=[0])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "sklearn_ann = SklearnANN(\n",
    "    epochs=50,\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5\n",
    ")\n",
    "# train model\n",
    "sklearn_ann = sklearn_ann.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    plot=True\n",
    ")\n",
    "# evaluate model\n",
    "ypred_ann_sklearn = sklearn_ann.evaluate(xtest_organ, ytest_organ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(sklearn_ann.log, index=[0])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteANN():\n",
    "    \n",
    "    def __init__(self, n_bits=2, epochs=50, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5):\n",
    "        self.n_bits = n_bits\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_layers = n_layers\n",
    "        self.neuron_factor = neuron_factor\n",
    "        self.model = None\n",
    "        self.fhe_circuit = None\n",
    "        self.log = {\n",
    "            'n_bits': n_bits,\n",
    "            'train': 0,\n",
    "            'compile': None,\n",
    "            'keygen': None,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None,\n",
    "            'accuracy': None,\n",
    "            'precision': None,\n",
    "            'recall': None,\n",
    "            'f1': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, plot=True):\n",
    "        # combine and reshape data\n",
    "        x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "        x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "        y_train = np.concatenate((y_train, y_val), axis=0).astype(np.int64)\n",
    "\n",
    "        # initialize model\n",
    "        self.model = NeuralNetClassifier(\n",
    "            lr = self.learning_rate,\n",
    "            max_epochs = self.epochs,\n",
    "            batch_size = 128,\n",
    "            callbacks = [EpochScoring(scoring='accuracy', name='train_acc', on_train=True)],\n",
    "            verbose = 0,\n",
    "            **{\n",
    "                'module__n_layers': self.n_layers,\n",
    "                'module__n_w_bits': self.n_bits,\n",
    "                'module__n_a_bits': self.n_bits,\n",
    "                'module__n_accum_bits': 16,\n",
    "                'module__n_hidden_neurons_multiplier': self.neuron_factor,\n",
    "                'optimizer__weight_decay': self.weight_decay,\n",
    "            }\n",
    "        )\n",
    "        # train model\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X=x_train, y=y_train)\n",
    "        self.log['train'] = time.time() - start_time\n",
    "        # compile model\n",
    "        start_time = time.time()\n",
    "        self.fhe_circuit = self.model.compile(x_train[:100])\n",
    "        self.log['compile'] = time.time() - start_time\n",
    "\n",
    "        # curve metrics\n",
    "        train_l = self.model.sklearn_model.history[:, 'train_loss']\n",
    "        val_l = self.model.sklearn_model.history[:, 'valid_loss']\n",
    "        train_score = self.model.sklearn_model.history[:, 'train_acc']\n",
    "        val_score = self.model.sklearn_model.history[:, 'valid_acc']\n",
    "        # plot results\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1]. set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X, y_true, fhe='simulate'):\n",
    "        # reshape data\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        # key generation\n",
    "        start_time = time.time()\n",
    "        self.fhe_circuit.keygen(force=True)\n",
    "        self.log['keygen'] = time.time() - start_time\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        y_pred = np.array([self.model.predict(X[[i]], fhe=fhe)[0] for i in tqdm(range(X.shape[0]))])\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "        # calculate metrics\n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            self.log['accuracy'] = round(accuracy_score(y_true, y_pred), 2)\n",
    "            self.log['precision'] = round(precision_score(y_true, y_pred), 2)\n",
    "            self.log['recall'] = round(recall_score(y_true, y_pred), 2)\n",
    "            self.log['f1'] = round(f1_score(y_true, y_pred), 2)\n",
    "        else:\n",
    "            self.log['accuracy'] = round(accuracy_score(y_true, y_pred), 2)\n",
    "            self.log['precision'] = round(precision_score(y_true, y_pred, average='macro'), 2)\n",
    "            self.log['recall'] = round(recall_score(y_true, y_pred, average='macro'), 2)\n",
    "            self.log['f1'] = round(f1_score(y_true, y_pred, average='macro'), 2)\n",
    "        # plot results\n",
    "        plot_classification_metrics(y_true, y_pred, plot_title=f'Concrete ANN (n_bits={self.n_bits})')\n",
    "        # return prediction labels\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "\n",
    "# initialize model\n",
    "concrete_ann = ConcreteANN(\n",
    "    n_bits=4,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5\n",
    ")\n",
    "# train model\n",
    "concrete_ann = concrete_ann.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    # initialize model\n",
    "    concrete_ann = ConcreteANN(\n",
    "        n_bits=n_bits,\n",
    "        epochs=50,\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=0.01,\n",
    "        n_layers=2,\n",
    "        neuron_factor=0.5\n",
    "    )\n",
    "    # train model\n",
    "    concrete_ann = concrete_ann.train(\n",
    "        x_train=xtrain_pneumonia,\n",
    "        y_train=ytrain_pneumonia,\n",
    "        x_val=xval_pneumonia,\n",
    "        y_val=yval_pneumonia,\n",
    "        plot=False\n",
    "    )\n",
    "    # evaluate model\n",
    "    ypred_concrete = concrete_ann.evaluate(xtest_pneumonia, ytest_pneumonia, fhe=fhe_mode)\n",
    "    # append results\n",
    "    results.append(concrete_ann.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "\n",
    "# initialize model\n",
    "concrete_ann = ConcreteANN(\n",
    "    n_bits=4,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.5,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.3\n",
    ")\n",
    "# train model\n",
    "concrete_ann = concrete_ann.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    # initialize model\n",
    "    concrete_ann = ConcreteANN(\n",
    "        n_bits=n_bits,\n",
    "        epochs=50,\n",
    "        learning_rate=0.0005,\n",
    "        weight_decay=0.5,\n",
    "        n_layers=2,\n",
    "        neuron_factor=0.3\n",
    "    )\n",
    "    # train model\n",
    "    concrete_ann = concrete_ann.train(\n",
    "        x_train=xtrain_breast,\n",
    "        y_train=ytrain_breast,\n",
    "        x_val=xval_breast,\n",
    "        y_val=yval_breast,\n",
    "        plot=False\n",
    "    )\n",
    "    # evaluate model\n",
    "    ypred_concrete = concrete_ann.evaluate(xtest_breast, ytest_breast, fhe=fhe_mode)\n",
    "    # append results\n",
    "    results.append(concrete_ann.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "\n",
    "# initialize model\n",
    "concrete_ann = ConcreteANN(\n",
    "    n_bits=4,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0005,\n",
    "    weight_decay=0.001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5\n",
    ")\n",
    "# train model\n",
    "concrete_ann = concrete_ann.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    # initialize model\n",
    "    concrete_ann = ConcreteANN(\n",
    "        n_bits=n_bits,\n",
    "        epochs=50,\n",
    "        learning_rate=0.0005,\n",
    "        weight_decay=0.001,\n",
    "        n_layers=2,\n",
    "        neuron_factor=0.5\n",
    "    )\n",
    "    # train model\n",
    "    concrete_ann = concrete_ann.train(\n",
    "        x_train=xtrain_organ,\n",
    "        y_train=ytrain_organ,\n",
    "        x_val=xval_organ,\n",
    "        y_val=yval_organ,\n",
    "        plot=False\n",
    "    )\n",
    "    # evaluate model\n",
    "    ypred_concrete = concrete_ann.evaluate(xtest_organ, ytest_organ, fhe=fhe_mode)\n",
    "    # append results\n",
    "    results.append(concrete_ann.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCNN():\n",
    "    def __init__(self, epochs=30, learning_rate=0.0001, weight_decay=0.0001):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.model = None\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None,\n",
    "            'accuracy': None,\n",
    "            'precision': None,\n",
    "            'recall': None,\n",
    "            'f1': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, plot=True):\n",
    "        # initialize model\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(in_features=432, out_features=len(np.unique(y_train)), bias=True)\n",
    "        )\n",
    "        # data to tensor\n",
    "        x_train, y_train = torch.tensor(x_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long).flatten()\n",
    "        x_val, y_val = torch.tensor(x_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.long).flatten()\n",
    "        # criterion\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        # train loop\n",
    "        train_l, val_l, train_score, val_score= [], [], [], []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            # prune model and fine-tune for last 5 epochs\n",
    "            if epoch == self.epochs - 5:\n",
    "                for layer in self.model:\n",
    "                    if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "                        torch.nn.utils.prune.l1_unstructured(layer, name='weight', amount=0.3)\n",
    "\n",
    "            # set model to train mode\n",
    "            self.model.train()\n",
    "            start_time = time.time()\n",
    "            for i in range(0, len(x_train), 1):\n",
    "                inputs, labels = x_train[i:i+1], y_train[i:i+1]\n",
    "                # forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.log['train'] += epoch_time\n",
    "\n",
    "            # set model to eval mode\n",
    "            self.model.eval()\n",
    "            # calculate curve metrics\n",
    "            with torch.no_grad():\n",
    "                train_outputs = self.model(x_train)\n",
    "                train_loss = criterion(train_outputs, y_train)\n",
    "                predicted = torch.argmax(train_outputs, dim=1)\n",
    "                train_accuracy = accuracy_score(predicted.detach().numpy(), y_train.detach().numpy())*100\n",
    "\n",
    "                val_outputs = self.model(x_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                predicted = torch.argmax(val_outputs, dim=1)\n",
    "                val_accuracy = accuracy_score(predicted.detach().numpy(), y_val.detach().numpy())*100\n",
    "\n",
    "            train_l.append(train_loss.item())\n",
    "            val_l.append(val_loss.item())\n",
    "            train_score.append(train_accuracy)\n",
    "            val_score.append(val_accuracy)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1]. set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        # make pruning permanent\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "                torch.nn.utils.prune.remove(layer, 'weight')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        # data to tensor\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # predict each sample individually\n",
    "            outputs = [self.model(X[[i]]) for i in tqdm(range(X.shape[0]))]\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "        # get binary predictions\n",
    "        y_pred = torch.argmax(torch.cat(outputs), dim=1).detach().numpy().flatten()\n",
    "        # calculate metrics\n",
    "        if len(np.unique(y_pred)) == 2:\n",
    "            self.log['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "            self.log['precision'] = precision_score(y_true, y_pred)\n",
    "            self.log['recall'] = recall_score(y_true, y_pred)\n",
    "            self.log['f1'] = f1_score(y_true, y_pred)\n",
    "        else:\n",
    "            self.log['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "            self.log['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "            self.log['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "            self.log['f1'] = f1_score(y_true, y_pred, average='macro')\n",
    "        # plot results\n",
    "        plot_classification_metrics(y_true, y_pred, plot_title='Torch CNN')\n",
    "        # return prediction labels\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "torch_cnn = TorchCNN(\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "# train model\n",
    "torch_cnn = torch_cnn.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    plot=True\n",
    ")\n",
    "# evaluate model\n",
    "ypred_torch = torch_cnn.evaluate(xtest_pneumonia, ytest_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(torch_cnn.log, index=[0])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "torch_cnn = TorchCNN(\n",
    "    epochs=30,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "# train model\n",
    "torch_cnn = torch_cnn.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    plot=True\n",
    ")\n",
    "# evaluate model\n",
    "ypred_torch = torch_cnn.evaluate(xtest_breast, ytest_breast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(torch_cnn.log, index=[0])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "torch_cnn = TorchCNN(\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "# train model\n",
    "torch_cnn = torch_cnn.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    plot=True\n",
    ")\n",
    "# evaluate model\n",
    "ypred_torch = torch_cnn.evaluate(xtest_organ, ytest_organ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(torch_cnn.log, index=[0])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteCNN():\n",
    "    def __init__(self, n_bits=2, epochs=30, learning_rate=0.0001, weight_decay=0.0001):\n",
    "        self.n_bits = n_bits\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.brevitas_model = None\n",
    "        self.concrete_model = None\n",
    "        self.log = {\n",
    "            'n_bits': n_bits,\n",
    "            'train': 0,\n",
    "            'compile': None,\n",
    "            'keygen': None,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None,\n",
    "            'accuracy': None,\n",
    "            'precision': None,\n",
    "            'recall': None,\n",
    "            'f1': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, plot=True):\n",
    "        # initialize model\n",
    "        self.brevitas_model = torch.nn.Sequential(\n",
    "            # entry point of a network has to be a QuantIdentity layer\n",
    "            # to quantize the inputs\n",
    "            brevitas.nn.QuantIdentity(bit_width=self.n_bits, return_quant_tensor=True),\n",
    "            # brevitas QuantConv2d layer\n",
    "            brevitas.nn.QuantConv2d(in_channels=1, out_channels=3, kernel_size=5, weight_bit_width=self.n_bits),\n",
    "            # brevitas QuantReLU layer\n",
    "            brevitas.nn.QuantReLU(bit_width=self.n_bits),\n",
    "            # torch AvgPool2d layer\n",
    "            torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            # after a PyTorch layer there needs to follow a QuantIdentity layer\n",
    "            # to make sure the inputs are quantized correctly\n",
    "            brevitas.nn.QuantIdentity(bit_width=self.n_bits, return_quant_tensor=True),\n",
    "            # PyTorch flatten layer\n",
    "            torch.nn.Flatten(),\n",
    "            # again a QuantIdentity layer after the PyTorch layer\n",
    "            brevitas.nn.QuantIdentity(bit_width=self.n_bits, return_quant_tensor=True),\n",
    "            # brevitas QuantLinear (output) layer\n",
    "            brevitas.nn.QuantLinear(in_features=432, out_features=len(np.unique(y_train)), bias=True, weight_bit_width=self.n_bits)\n",
    "        )\n",
    "        # data to tensor\n",
    "        x_train, y_train = torch.tensor(x_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long).flatten()\n",
    "        x_val, y_val = torch.tensor(x_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.long).flatten()\n",
    "        # criterion\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(self.brevitas_model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        # train loop\n",
    "        train_l, val_l, train_score, val_score= [], [], [], []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            # prune model and fine-tune for last 5 epochs\n",
    "            if epoch == self.epochs - 5:\n",
    "                for layer in self.brevitas_model:\n",
    "                    if isinstance(layer, (brevitas.nn.QuantConv2d, brevitas.nn.QuantLinear)):\n",
    "                        torch.nn.utils.prune.l1_unstructured(layer, name='weight', amount=0.3)\n",
    "            \n",
    "            # set model to train mode\n",
    "            self.brevitas_model.train()\n",
    "            start_time = time.time()\n",
    "            for i in range(0, len(x_train), 1):\n",
    "                inputs, labels = x_train[i:i+1], y_train[i:i+1]\n",
    "                # forward pass\n",
    "                outputs = self.brevitas_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.log['train'] += epoch_time\n",
    "\n",
    "            # set model to eval mode\n",
    "            self.brevitas_model.eval()\n",
    "            # calculate curve metrics\n",
    "            with torch.no_grad():\n",
    "                train_outputs = self.brevitas_model(x_train)\n",
    "                train_loss = criterion(train_outputs, y_train)\n",
    "                predicted = torch.argmax(train_outputs, dim=1)\n",
    "                train_accuracy = accuracy_score(predicted.detach().numpy(), y_train.detach().numpy())*100\n",
    "\n",
    "                val_outputs = self.brevitas_model(x_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                predicted = torch.argmax(val_outputs, dim=1)\n",
    "                val_accuracy = accuracy_score(predicted.detach().numpy(), y_val.detach().numpy())*100\n",
    "\n",
    "            train_l.append(train_loss.item())\n",
    "            val_l.append(val_loss.item())\n",
    "            train_score.append(train_accuracy)\n",
    "            val_score.append(val_accuracy)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1]. set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        # make pruning permanent\n",
    "        for layer in self.brevitas_model:\n",
    "            if isinstance(layer, (brevitas.nn.QuantConv2d, brevitas.nn.QuantLinear)):\n",
    "                torch.nn.utils.prune.remove(layer, 'weight')\n",
    "\n",
    "        # compile model\n",
    "        start_time = time.time()\n",
    "        self.concrete_model = compile_brevitas_qat_model(self.brevitas_model, x_train[:100], rounding_threshold_bits=6)\n",
    "        self.log['compile'] = time.time() - start_time\n",
    "\n",
    "        bit_width = self.concrete_model.fhe_circuit.graph.maximum_integer_bit_width()\n",
    "        print(f'WARNING: Maximum bit-width of 16 exceeded ({bit_width})') if bit_width > 16 else None\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X, y_true, fhe='simulate'):\n",
    "        # keygen\n",
    "        start_time = time.time()\n",
    "        self.concrete_model.fhe_circuit.keygen(force=True)\n",
    "        self.log['keygen'] = time.time() - start_time\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # predict each sample individually\n",
    "            outputs = [self.concrete_model.forward(X[[i]], fhe=fhe) for i in tqdm(range(X.shape[0]))]\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "        # get binary predictions from numpy outputs\n",
    "        y_pred = np.argmax(np.concatenate(outputs), axis=1)\n",
    "        # calculate metrics\n",
    "        if len(np.unique(y_pred)) == 2:\n",
    "            self.log['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "            self.log['precision'] = precision_score(y_true, y_pred)\n",
    "            self.log['recall'] = recall_score(y_true, y_pred)\n",
    "            self.log['f1'] = f1_score(y_true, y_pred)\n",
    "        else:\n",
    "            self.log['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "            self.log['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "            self.log['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "            self.log['f1'] = f1_score(y_true, y_pred, average='macro')\n",
    "        # plot results\n",
    "        plot_classification_metrics(y_true, y_pred, plot_title=f'Concrete CNN (n_bits={self.n_bits})')\n",
    "        # return prediction labels\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "\n",
    "# initialize model\n",
    "concrete_cnn = ConcreteCNN(\n",
    "    n_bits=4,\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001\n",
    ")\n",
    "# train model\n",
    "concrete_cnn = concrete_cnn.train(\n",
    "        x_train=xtrain_pneumonia,\n",
    "        y_train=ytrain_pneumonia,\n",
    "        x_val=xval_pneumonia,\n",
    "        y_val=yval_pneumonia,\n",
    "        plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    # initialize model\n",
    "    concrete_cnn = ConcreteCNN(\n",
    "        n_bits=n_bits,\n",
    "        epochs=30,\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=0.0001\n",
    "    )\n",
    "    # train model\n",
    "    concrete_cnn = concrete_cnn.train(\n",
    "        x_train=xtrain_pneumonia,\n",
    "        y_train=ytrain_pneumonia,\n",
    "        x_val=xval_pneumonia,\n",
    "        y_val=yval_pneumonia,\n",
    "        plot=False\n",
    "    )\n",
    "    # evaluate model\n",
    "    ypred_concrete = concrete_cnn.evaluate(xtest_pneumonia, ytest_pneumonia, fhe=fhe_mode)\n",
    "    # append results\n",
    "    results.append(concrete_cnn.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "\n",
    "# initialize model\n",
    "concrete_cnn = ConcreteCNN(\n",
    "    n_bits=4,\n",
    "    epochs=30,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "# train model\n",
    "concrete_cnn = concrete_cnn.train(\n",
    "        x_train=xtrain_breast,\n",
    "        y_train=ytrain_breast,\n",
    "        x_val=xval_breast,\n",
    "        y_val=yval_breast,\n",
    "        plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    # initialize model\n",
    "    concrete_cnn = ConcreteCNN(\n",
    "        n_bits=n_bits,\n",
    "        epochs=30,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    # train model\n",
    "    concrete_cnn = concrete_cnn.train(\n",
    "        x_train=xtrain_breast,\n",
    "        y_train=ytrain_breast,\n",
    "        x_val=xval_breast,\n",
    "        y_val=yval_breast,\n",
    "        plot=False\n",
    "    )\n",
    "    # evaluate model\n",
    "    ypred_concrete = concrete_cnn.evaluate(xtest_breast, ytest_breast, fhe=fhe_mode)\n",
    "    # append results\n",
    "    results.append(concrete_cnn.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune model\n",
    "\n",
    "# initialize model\n",
    "concrete_cnn = ConcreteCNN(\n",
    "    n_bits=4,\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "# train model\n",
    "concrete_cnn = concrete_cnn.train(\n",
    "        x_train=xtrain_organ,\n",
    "        y_train=ytrain_organ,\n",
    "        x_val=xval_organ,\n",
    "        y_val=yval_organ,\n",
    "        plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    # initialize model\n",
    "    concrete_cnn = ConcreteCNN(\n",
    "        n_bits=n_bits,\n",
    "        epochs=30,\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "    # train model\n",
    "    concrete_cnn = concrete_cnn.train(\n",
    "        x_train=xtrain_organ,\n",
    "        y_train=ytrain_organ,\n",
    "        x_val=xval_organ,\n",
    "        y_val=yval_organ,\n",
    "        plot=False\n",
    "    )\n",
    "    # evaluate model\n",
    "    ypred_concrete = concrete_cnn.evaluate(xtest_organ, ytest_organ, fhe=fhe_mode)\n",
    "    # append results\n",
    "    results.append(concrete_cnn.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
