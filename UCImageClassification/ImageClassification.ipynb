{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Classification\n",
    "\n",
    "**Company Use Case:**\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "The following Notebook will show two approaches to do this with ConcreteML.\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "The data used is provided by MedMNIST v2, a comprehensive collection of standardized biomedical images. It encompasses 12 datasets for 2D and 6 for 3D images, pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with corresponding classification labels. With 708,069 2D images and 9,998 3D images, it supports various classification tasks, from binary/multi-class to ordinal regression and multi-label, catering to biomedical image analysis, computer vision, and machine learning research and education.\n",
    "\n",
    "https://medmnist.com/\n",
    "\n",
    "**Dataset  1:**\n",
    "\n",
    "_OCTMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The OCTMNIST is based on a prior dataset of 109,309 valid optical coherence tomography (OCT) images for retinal diseases. The dataset is comprised of 4 diagnosis categories, leading to a multi-class classification task. We split the source training set with a ratio of 9:1 into training and validation set, and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−1,536)×(277−512). We center-crop the images and resize them into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/octmnist.npz?download=1\n",
    "\n",
    "**Dataset 2:**\n",
    "\n",
    "_PneumoniaMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/pneumoniamnist.npz?download=1\n",
    "\n",
    "**Dataset 2:**\n",
    "\n",
    "<span style=\"color:red\">_NAME_</span> (Maybe BloodMNIST)\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "<span style=\"color:red\">TEXT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHE Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'simulate'\n",
    "\n",
    "# mode = 'execute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# MedMNIST\n",
    "import medmnist\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# XGBoost\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, vgg16\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Logging (KICK AFTER BEING DONE)\n",
    "###############################################################\n",
    "\n",
    "def log_parameters(dataset, epochs, learning_rate, weight_decay, model, train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    if os.path.isfile(f'{dataset}_Experiments.csv'):\n",
    "        df = pd.read_csv(f'{dataset}_Experiments.csv')\n",
    "        df = df.drop_duplicates()\n",
    "        # append new row\n",
    "        df = pd.concat([df, pd.DataFrame({\n",
    "            'epochs': [epochs],\n",
    "            'learning_rate': [learning_rate],\n",
    "            'weight_decay': [weight_decay],\n",
    "            'model': [str(model)],\n",
    "            'final_train_loss': [train_losses[-1]],\n",
    "            'final_train_accuracy': [train_accuracies[-1]],\n",
    "            'final_val_loss': [val_losses[-1]],\n",
    "            'final_val_accuracy': [val_accuracies[-1]]\n",
    "            })])\n",
    "        # save csv\n",
    "        df.to_csv(f'{dataset}_Experiments.csv', index=False)\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "        'epochs': [epochs],\n",
    "        'learning_rate': [learning_rate],\n",
    "        'weight_decay': [weight_decay],\n",
    "        'model': [str(model)],\n",
    "        'final_train_loss': [train_losses[-1]],\n",
    "        'final_train_accuracy': [train_accuracies[-1]],\n",
    "        'final_val_loss': [val_losses[-1]],\n",
    "        'final_val_accuracy': [val_accuracies[-1]]\n",
    "        })\n",
    "        # save csv\n",
    "        df.to_csv(f'{dataset}_Experiments.csv', index=False)\n",
    "    \n",
    "    return 'Parameters Logged!'\n",
    "\n",
    "###############################################################\n",
    "# Load Data\n",
    "###############################################################\n",
    "\n",
    "def load_data(dataset):\n",
    "\n",
    "    '''\n",
    "    Load data from MedMNIST\n",
    "\n",
    "    Input:\n",
    "        dataset (str): name of dataset\n",
    "\n",
    "    Output:\n",
    "        X_grayscale (tuple): (xtrain, xval, xtest) grayscale images\n",
    "        X_rgb (tuple): (xtrain, xval, xtest) rgb images\n",
    "        y (tuple): (ytrain, yval, ytest) labels\n",
    "    '''\n",
    "\n",
    "    # initialize DataClass\n",
    "    DataClass = getattr(medmnist, dataset)\n",
    "    # download data\n",
    "    train_dataset = DataClass(split='train',download=True)\n",
    "    eval_dataset = DataClass(split='val', download=True)\n",
    "    test_dataset = DataClass(split='test', download=True)\n",
    "    # to numpy array\n",
    "    xtrain, ytrain = train_dataset.imgs, train_dataset.labels\n",
    "    xval, yval = eval_dataset.imgs, eval_dataset.labels\n",
    "    xtest, ytest = test_dataset.imgs, test_dataset.labels\n",
    "    # expand dimension (grayscale)\n",
    "    # (channels=1, height=28, width=28)\n",
    "    xtrain_gray = np.expand_dims(xtrain, axis=1)\n",
    "    xval_gray = np.expand_dims(xval, axis=1)\n",
    "    xtest_gray = np.expand_dims(xtest, axis=1)\n",
    "    # expand dimension (rgb) - needed for pretrained models\n",
    "    # (channels=3, height=28, width=28)\n",
    "    xtrain_rgb = np.repeat(xtrain, 3, axis=1)\n",
    "    xval_rgb = np.repeat(xval, 3, axis=1)\n",
    "    xtest_rgb = np.repeat(xtest, 3, axis=1)\n",
    "\n",
    "    X_grayscale = (xtrain_gray, xval_gray, xtest_gray)\n",
    "    X_rgb = (xtrain_rgb, xval_rgb, xtest_rgb)\n",
    "    y = (ytrain, yval, ytest)\n",
    "\n",
    "    return X_grayscale, X_rgb, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'PneumoniaMNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dataset=dataset)\n",
    "\n",
    "xtrain_gray, xval_gray, xtest_gray = data[0]\n",
    "xtrain_rgb, xval_rgb, xtest_rgb = data[1]\n",
    "ytrain, yval, ytest = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample images of each class\n",
    "n_classes = len(np.unique(ytrain))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n_classes, figsize=(10, 5))\n",
    "for i in range(n_classes):\n",
    "    ax[i].imshow(xtrain_gray[ytrain.flatten()==i][0][0], cmap='gray')\n",
    "    ax[i].set_title(f'Class {i}')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(model, x_train, x_val, y_train, y_val, epochs=20, learning_rate=0.0001, weight_decay=0.0001, plot=True):\n",
    "    '''\n",
    "    Train Torch Model\n",
    "\n",
    "    Input:\n",
    "        model (torch model): model to train\n",
    "        x_train (np array): training data\n",
    "        x_val (np array): validation data\n",
    "        y_train (np array): training labels\n",
    "        y_val (np array): validation labels\n",
    "        epochs (int): number of epochs\n",
    "        learning_rate (float): learning rate\n",
    "        weight_decay (float): weight decay\n",
    "        plot (bool): plot metrics\n",
    "\n",
    "    Output:\n",
    "        model (torch model): trained model\n",
    "    '''\n",
    "\n",
    "    # data to tensor\n",
    "    x_train, y_train = torch.tensor(x_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float)\n",
    "    x_val, y_val = torch.tensor(x_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.float)\n",
    "\n",
    "    # criterion and optimizer\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # create lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # train model\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # training\n",
    "        model.train()\n",
    "        for i in range(0, len(x_train), 1):\n",
    "            inputs = x_train[i:i+1]\n",
    "            labels = y_train[i:i+1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # log metrics\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(x_train)\n",
    "            train_loss = criterion(train_outputs, y_train)\n",
    "            predicted = torch.round(train_outputs)\n",
    "            train_accuracy = accuracy_score(predicted.detach().numpy(), y_train.detach().numpy())*100\n",
    "\n",
    "            val_outputs = model(x_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "            predicted = torch.round(val_outputs)\n",
    "            val_accuracy = accuracy_score(predicted.detach().numpy(), y_val.detach().numpy())*100\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # plot metrics\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].plot(train_losses, label='Training Loss')\n",
    "        ax[0].plot(val_losses, label='Validation Loss')\n",
    "        ax[0].set_xticks(np.arange(0, epochs, 1))\n",
    "        ax[0]. set_xlabel('Epochs')\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].plot(train_accuracies, label='Training Accuracy')\n",
    "        ax[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "        ax[1].set_xticks(np.arange(0, epochs, 1))\n",
    "        ax[1]. set_xlabel('Epochs')\n",
    "        ax[1].set_title('Accuracy')\n",
    "        ax[1].legend()\n",
    "        plt.show()\n",
    "\n",
    "    log_parameters(dataset, epochs, learning_rate, weight_decay, model, train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(ytrain))\n",
    "\n",
    "torch_model = torch.nn.Sequential(\n",
    "      torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "      torch.nn.Conv2d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "      torch.nn.Flatten(),\n",
    "      torch.nn.Linear(in_features=64, out_features=1, bias=True),\n",
    "      torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "summary(torch_model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_torch(torch_model, xtrain_gray, xval_gray, ytrain, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'{dataset}_Experiments.csv').sort_values(by='final_val_accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Feature Extractor (Transfer Learning) & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check how image looks like after resnet50 preprocessing\n",
    "# x = train_dataset[0][0][0]\n",
    "# plt.imshow(x, cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "# # resnet50 preprocessing\n",
    "# preprocess = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[.5], std=[.5])\n",
    "# ])\n",
    "\n",
    "# # resnet50 preprocessing\n",
    "# x = preprocess(x)\n",
    "# plt.imshow(x[0], cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
