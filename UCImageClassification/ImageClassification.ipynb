{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Classification\n",
    "\n",
    "**Company Use Case:**\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "The following Notebook will show 3 approaches to do this with ConcreteML.\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "The data used is provided by MedMNIST v2, a comprehensive collection of standardized biomedical images. It encompasses 12 datasets for 2D and 6 for 3D images, pre-processed into 28 x 28 (2D) or 28 x 28 x 28 (3D) with corresponding classification labels. With 708,069 2D images and 9,998 3D images, it supports various classification tasks, from binary/multi-class to ordinal regression and multi-label, catering to biomedical image analysis, computer vision, and machine learning research and education.\n",
    "\n",
    "https://medmnist.com/\n",
    "\n",
    "**Dataset  1:**\n",
    "\n",
    "_PneumoniaMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The PneumoniaMNIST is based on a prior dataset of 5,856 pediatric chest X-Ray images. The task is binary-class classification of pneumonia against normal. We split the source training set with a ratio of 9:1 into training and validation set and use its source validation set as the test set. The source images are gray-scale, and their sizes are (384−2,916)×(127−2,713). We center-crop the images and resize them into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/pneumoniamnist.npz?download=1\n",
    "\n",
    "**Dataset 2:**\n",
    "\n",
    "_BreastMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/breastmnist.npz?download=1\n",
    "\n",
    "**Dataset 3:**\n",
    "\n",
    "_OrganCMNIST_\n",
    "\n",
    "MedMNIST Description:\n",
    "\n",
    "The OrganCMNIST is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS). It is renamed from OrganMNIST_Coronal (in MedMNIST v1) for simplicity. We use bounding-box annotations of 11 body organs from another study to obtain the organ labels. Hounsfield-Unit (HU) of the 3D images are transformed into gray-scale with an abdominal window. We crop 2D images from the center slices of the 3D bounding boxes in coronal views (planes). The images are resized into 1×28×28 to perform multi-class classification of 11 body organs. 115 and 16 CT scans from the source training set are used as training and validation set, respectively. The 70 CT scans from the source test set are treated as the test set.\n",
    "\n",
    "https://zenodo.org/records/6496656/files/organcmnist.npz?download=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHE Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'simulate'\n",
    "\n",
    "# mode = 'execute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# MedMNIST\n",
    "import medmnist\n",
    "\n",
    "# SciKit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# Imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# XGBoost\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchsummary import summary\n",
    "\n",
    "# ConcreteML\n",
    "from concrete.ml.sklearn import XGBClassifier as ConcreteXGBClassifier\n",
    "from concrete.ml.sklearn import NeuralNetClassifier\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "\n",
    "# Skorch\n",
    "from skorch.callbacks import EpochScoring\n",
    "\n",
    "# Brevitas\n",
    "import brevitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pep8 linting\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc7eb1b1050>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Load Data\n",
    "################################################################################\n",
    "\n",
    "def load_data(dataset, smote=True):\n",
    "\n",
    "    '''\n",
    "    Load data from MedMNIST\n",
    "\n",
    "    Input:\n",
    "        dataset (str): name of dataset\n",
    "\n",
    "    Output:\n",
    "        X_grayscale (tuple): (xtrain, xval, xtest) grayscale images\n",
    "        X_rgb (tuple): (xtrain, xval, xtest) rgb images\n",
    "        y (tuple): (ytrain, yval, ytest) labels\n",
    "    '''\n",
    "\n",
    "    # initialize DataClass\n",
    "    DataClass = getattr(medmnist, dataset)\n",
    "    # download data\n",
    "    train_dataset = DataClass(split='train',download=True)\n",
    "    eval_dataset = DataClass(split='val', download=True)\n",
    "    test_dataset = DataClass(split='test', download=True)\n",
    "    # to numpy array\n",
    "    x = np.concatenate((train_dataset.imgs, eval_dataset.imgs, test_dataset.imgs), axis=0)\n",
    "    y = np.concatenate((train_dataset.labels, eval_dataset.labels, test_dataset.labels), axis=0).flatten()\n",
    "    # split into train, val, test=100 images\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=100, stratify=y)\n",
    "    xtrain, xval, ytrain, yval = train_test_split(xtrain, ytrain, test_size=0.2, stratify=ytrain)\n",
    "    # Smote\n",
    "    # if smote:\n",
    "    #     smote = SMOTE()\n",
    "    #     xtrain, ytrain = smote.fit_resample(xtrain.reshape(xtrain.shape[0], -1), ytrain)\n",
    "    #     xtrain = xtrain.reshape(xtrain.shape[0], 28, 28)\n",
    "    # expand dimension (grayscale)\n",
    "    # (channels=1, height=28, width=28)\n",
    "    xtrain = np.expand_dims(xtrain, axis=1)\n",
    "    xval = np.expand_dims(xval, axis=1)\n",
    "    xtest = np.expand_dims(xtest, axis=1)\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    return xtrain, xval, xtest, ytrain, yval, ytest\n",
    "\n",
    "################################################################################\n",
    "# Preprocess Data\n",
    "################################################################################\n",
    "\n",
    "def plot_images(x_train, y_train, dataset):\n",
    "\n",
    "    # plot sample images of each class\n",
    "    n_classes = len(np.unique(y_train))\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=n_classes, figsize=(10, 5))\n",
    "    for i in range(n_classes):\n",
    "        ax[i].imshow(x_train[y_train.flatten()==i][0][0], cmap='gray')\n",
    "        ax[i].set_title(f'Class {i}')\n",
    "        ax[i].axis('off')\n",
    "    fig.suptitle(dataset)\n",
    "    plt.show()\n",
    "\n",
    "################################################################################\n",
    "# Plot Classification Metrics\n",
    "################################################################################\n",
    "\n",
    "def plot_classification_metrics(\n",
    "    y_true: np.array,\n",
    "    y_pred: np.array,\n",
    "    plot_title: str = None\n",
    "    ):\n",
    "\n",
    "    '''\n",
    "    Plots Classification Metrics\n",
    "\n",
    "    Input:\n",
    "      y_true = ground truth labels\n",
    "      y_pred = prediction labels\n",
    "      plot_title = title for results plot (optional)\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(5, 5))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        accuracy = round(accuracy_score(y_true, y_pred),2)\n",
    "        precision = round(precision_score(y_true, y_pred),2)\n",
    "        recall = round(recall_score(y_true, y_pred),2)\n",
    "        f1 = round(f1_score(y_true, y_pred),2)\n",
    "        roc_auc = round(roc_auc_score(y_true, y_pred), 2)\n",
    "        ax[0].bar(['Accuracy', 'Precision', 'Recall', 'F1', 'RocAuc'], [accuracy, precision, recall, f1, roc_auc])\n",
    "    else:\n",
    "        accuracy = round(accuracy_score(y_true, y_pred),2)\n",
    "        precision = round(precision_score(y_true, y_pred, average='macro'),2)\n",
    "        recall = round(recall_score(y_true, y_pred, average='macro'),2)\n",
    "        f1 = round(f1_score(y_true, y_pred, average='macro'),2)\n",
    "        ax[0].bar(['Accuracy', 'Precision', 'Recall', 'F1'], [accuracy, precision, recall, f1])\n",
    "\n",
    "\n",
    "    # barchart of metrics for each classifier\n",
    "    ax[0].set_title('Classifier Metrics')\n",
    "    ax[0].set_ylim(0,1)\n",
    "    ax[0].bar_label(ax[0].containers[0], label_type='center')\n",
    "\n",
    "    # confusion matrix for each classifier\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot(ax=ax[1], cmap='Blues', colorbar=False)\n",
    "    ax[1].set_title('Classifier Confusion Matrix')\n",
    "\n",
    "    plt.suptitle(plot_title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dataset='PneumoniaMNIST')\n",
    "\n",
    "xtrain_pneumonia, xval_pneumonia, xtest_pneumonia, ytrain_pneumonia, yval_pneumonia, ytest_pneumonia = data\n",
    "\n",
    "data = load_data(dataset='BreastMNIST')\n",
    "\n",
    "xtrain_breast, xval_breast, xtest_breast, ytrain_breast, yval_breast, ytest_breast = data\n",
    "\n",
    "data = load_data(dataset='OrganCMNIST')\n",
    "\n",
    "xtrain_organ, xval_organ, xtest_organ, ytrain_organ, yval_organ, ytest_organ = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Grayscale Shape PneumoniaMNIST: {xtrain_pneumonia.shape}')\n",
    "print('\\n')\n",
    "print(f'Grayscale Shape BreastMNIST: {xtrain_breast.shape}')\n",
    "print('\\n')\n",
    "print(f'Grayscale Shape OrganCMNIST: {xtrain_organ.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(xtrain_pneumonia, ytrain_pneumonia, dataset='PneumoniaMNIST')\n",
    "plot_images(xtrain_breast, ytrain_breast, dataset='BreastMNIST')\n",
    "plot_images(xtrain_organ, ytrain_organ, dataset='OrganCMNIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize grayscale images for generic CNN\n",
    "\n",
    "# PneumoniaMNIST\n",
    "xtrain_pneumonia, xval_pneumonia, xtest_pneumonia = xtrain_pneumonia/255, xval_pneumonia/255, xtest_pneumonia/255\n",
    "\n",
    "# BreastMNIST\n",
    "xtrain_breast, xval_breast, xtest_breast = xtrain_breast/255, xval_breast/255, xtest_breast/255\n",
    "\n",
    "# OrganCMNIST\n",
    "xtrain_organ, xval_organ, xtest_organ = xtrain_organ/255, xval_organ/255, xtest_organ/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnANN():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=100, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5, plot=True):\n",
    "        # reshape data\n",
    "        x_train, x_val = x_train.reshape(x_train.shape[0], -1), x_val.reshape(x_val.shape[0], -1)\n",
    "        # initialize model\n",
    "        self.model = MLPClassifier(\n",
    "            hidden_layer_sizes=tuple([int(x_train.shape[1] * neuron_factor) for i in range(n_layers)]),\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=learning_rate,\n",
    "            alpha=weight_decay,\n",
    "            batch_size=128\n",
    "        )\n",
    "        # train model\n",
    "        train_l, val_l, train_score, val_score = [], [], [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            start_time = time.time()\n",
    "            self.model.partial_fit(x_train, y_train, classes=np.unique(y_train))\n",
    "            self.log['train'] += time.time() - start_time\n",
    "            # curve metrics\n",
    "            pred_train, pred_val  = self.model.predict_proba(x_train), self.model.predict_proba(x_val)\n",
    "            train_l.append(log_loss(y_train, pred_train, labels=np.unique(y_train)))\n",
    "            val_l.append(log_loss(y_val, pred_val, labels=np.unique(y_val)))\n",
    "            train_score.append(accuracy_score(y_train, np.argmax(pred_train, axis=1)))\n",
    "            val_score.append(accuracy_score(y_val, np.argmax(pred_val, axis=1)))\n",
    "\n",
    "        # plot results\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1].set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        # reshape data for sklearn\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "sklearn_ann = SklearnANN()\n",
    "\n",
    "# train model\n",
    "sklearn_ann.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5,\n",
    "    plot=True\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_ann_sklearn = sklearn_ann.evaluate(xtest_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_true=ytest_pneumonia, y_pred=ypred_ann_sklearn, plot_title='Sklearn ANN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_results = pd.DataFrame(sklearn_ann.log, index=[0])\n",
    "sklearn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28*28*0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "sklearn_ann = SklearnANN()\n",
    "\n",
    "# train model\n",
    "sklearn_ann.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5,\n",
    "    plot=True\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_ann_sklearn = sklearn_ann.evaluate(xtest_breast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_true=ytest_breast, y_pred=ypred_ann_sklearn, plot_title='Sklearn ANN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_results = pd.DataFrame(sklearn_ann.log, index=[0])\n",
    "sklearn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "sklearn_ann = SklearnANN()\n",
    "\n",
    "# train model\n",
    "sklearn_ann.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5,\n",
    "    plot=True\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_ann_sklearn = sklearn_ann.evaluate(xtest_organ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_true=ytest_organ, y_pred=ypred_ann_sklearn, plot_title='Sklearn ANN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_results = pd.DataFrame(sklearn_ann.log, index=[0])\n",
    "sklearn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteANN():\n",
    "    \n",
    "    def __init__(self, n_bits=2):\n",
    "        self.n_bits = n_bits\n",
    "        self.model = None\n",
    "        self.fhe_circuit = None\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'compile': None,\n",
    "            'keygen': None,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=100, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5, plot=True):\n",
    "        # combine and reshape data\n",
    "        x_train = np.concatenate((x_train, x_val), axis=0)\n",
    "        x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "        y_train = np.concatenate((y_train, y_val), axis=0).astype(np.int64)\n",
    "\n",
    "        # initialize model\n",
    "        self.model = NeuralNetClassifier(\n",
    "            lr=learning_rate,\n",
    "            max_epochs=epochs,\n",
    "            batch_size=128,\n",
    "            callbacks=[EpochScoring(scoring='accuracy', name='train_acc', on_train=True)],\n",
    "            verbose=0,\n",
    "            **{\n",
    "                'module__n_layers': n_layers,\n",
    "                'module__n_w_bits': self.n_bits,\n",
    "                'module__n_a_bits': self.n_bits,\n",
    "                'module__n_accum_bits': 16,\n",
    "                'module__n_hidden_neurons_multiplier': neuron_factor,\n",
    "                'optimizer__weight_decay': weight_decay,\n",
    "            }\n",
    "        )\n",
    "        # train model\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X=x_train, y=y_train)\n",
    "        self.log['train'] = time.time() - start_time\n",
    "        clear_output()\n",
    "        # compile model\n",
    "        start_time = time.time()\n",
    "        self.fhe_circuit = self.model.compile(x_train[:100])\n",
    "        self.log['compile'] = time.time() - start_time\n",
    "\n",
    "        # curve metrics\n",
    "        train_l = self.model.sklearn_model.history[:, 'train_loss']\n",
    "        val_l = self.model.sklearn_model.history[:, 'valid_loss']\n",
    "        train_score = self.model.sklearn_model.history[:, 'train_acc']\n",
    "        val_score = self.model.sklearn_model.history[:, 'valid_acc']\n",
    "        # plot results\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1]. set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X, fhe='simulate'):\n",
    "        # reshape data\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        # key generation\n",
    "        start_time = time.time()\n",
    "        self.fhe_circuit.keygen(force=True)\n",
    "        self.log['keygen'] = time.time() - start_time\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        y_pred = np.array([self.model.predict(X[[i]], fhe=fhe)[0] for i in tqdm(range(X.shape[0]))])\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "concrete_ann = ConcreteANN(n_bits=2)\n",
    "\n",
    "# train model\n",
    "concrete_ann.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5,\n",
    "    plot=True\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_ann_concrete = concrete_ann.evaluate(xtest_pneumonia, fhe=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_true=ytest_pneumonia, y_pred=ypred_ann_concrete, plot_title='Concrete ANN (2-bit)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    \n",
    "    concrete_ann = ConcreteANN(n_bits=n_bits)\n",
    "    concrete_ann.train(x_train=xtrain_pneumonia, y_train=ytrain_pneumonia, x_val=xval_pneumonia, y_val=yval_pneumonia, epochs=50, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5, plot=False)\n",
    "    ypred_concrete = concrete_ann.evaluate(xtest_pneumonia, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_pneumonia, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_pneumonia, ypred_concrete),\n",
    "        'recall':     recall_score(ytest_pneumonia, ypred_concrete),\n",
    "        'f1':         f1_score(ytest_pneumonia, ypred_concrete)\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_ann.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_pneumonia, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "concrete_ann = ConcreteANN(n_bits=2)\n",
    "\n",
    "# train model\n",
    "concrete_ann.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5,\n",
    "    plot=True\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_ann_concrete = concrete_ann.evaluate(xtest_breast, fhe=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_true=ytest_breast, y_pred=ypred_ann_concrete, plot_title='Concrete ANN (2 n_bits)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    concrete_ann = ConcreteANN(n_bits=n_bits)\n",
    "    concrete_ann.train(x_train=xtrain_breast, y_train=ytrain_breast, x_val=xval_breast, y_val=yval_breast, epochs=50, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5, plot=False)\n",
    "    ypred_concrete = concrete_ann.evaluate(xtest_breast, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_breast, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_breast, ypred_concrete),\n",
    "        'recall':     recall_score(ytest_breast, ypred_concrete),\n",
    "        'f1':         f1_score(ytest_breast, ypred_concrete)\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_ann.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_breast, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "concrete_ann = ConcreteANN(n_bits=2)\n",
    "\n",
    "# train model\n",
    "concrete_ann.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001,\n",
    "    n_layers=2,\n",
    "    neuron_factor=0.5,\n",
    "    plot=True\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_ann_concrete = concrete_ann.evaluate(xtest_organ, fhe=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(y_true=ytest_organ, y_pred=ypred_ann_concrete, plot_title='Concrete ANN (2 n_bits)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    concrete_ann = ConcreteANN(n_bits=n_bits)\n",
    "    concrete_ann.train(x_train=xtrain_organ, y_train=ytrain_organ, x_val=xval_organ, y_val=yval_organ, epochs=50, learning_rate=0.0001, weight_decay=0.0001, n_layers=2, neuron_factor=0.5, plot=False)\n",
    "    ypred_concrete = concrete_ann.evaluate(xtest_organ, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_organ, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_organ, ypred_concrete, average='macro'),\n",
    "        'recall':     recall_score(ytest_organ, ypred_concrete, average='macro'),\n",
    "        'f1':         f1_score(ytest_organ, ypred_concrete, average='macro')\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_ann.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_organ, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCNN():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=30, learning_rate=0.0001, weight_decay=0.0001, plot=True):\n",
    "        # data to tensor\n",
    "        x_train, y_train = torch.tensor(x_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long).flatten()\n",
    "        x_val, y_val = torch.tensor(x_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.long).flatten()\n",
    "        # criterion\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # train loop\n",
    "        train_l, val_l, train_score, val_score= [], [], [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            # set model to train mode\n",
    "            self.model.train()\n",
    "            start_time = time.time()\n",
    "            for i in range(0, len(x_train), 1):\n",
    "                inputs, labels = x_train[i:i+1], y_train[i:i+1]\n",
    "                # forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.log['train'] += epoch_time\n",
    "\n",
    "            # set model to eval mode\n",
    "            self.model.eval()\n",
    "            # calculate curve metrics\n",
    "            with torch.no_grad():\n",
    "                train_outputs = self.model(x_train)\n",
    "                train_loss = criterion(train_outputs, y_train)\n",
    "                predicted = torch.argmax(train_outputs, dim=1)\n",
    "                train_accuracy = accuracy_score(predicted.detach().numpy(), y_train.detach().numpy())*100\n",
    "\n",
    "                val_outputs = self.model(x_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                predicted = torch.argmax(val_outputs, dim=1)\n",
    "                val_accuracy = accuracy_score(predicted.detach().numpy(), y_val.detach().numpy())*100\n",
    "\n",
    "            train_l.append(train_loss.item())\n",
    "            val_l.append(val_loss.item())\n",
    "            train_score.append(train_accuracy)\n",
    "            val_score.append(val_accuracy)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1]. set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        # data to tensor\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        # set model to eval mode\n",
    "        self.model.eval()\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # predict each sample individually\n",
    "            outputs = [self.model(X[[i]]) for i in tqdm(range(X.shape[0]))]\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "        # get binary predictions\n",
    "        y_pred = torch.argmax(torch.cat(outputs), dim=1).detach().numpy().flatten()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(ytrain_pneumonia))\n",
    "\n",
    "torch_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Conv2d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(in_features=64, out_features=n_classes, bias=True)\n",
    "    )\n",
    "\n",
    "summary(torch_model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "torch_cnn = TorchCNN(model=torch_model)\n",
    "\n",
    "# train model\n",
    "torch_cnn = torch_cnn.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_torch = torch_cnn.evaluate(xtest_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(ytest_pneumonia, ypred_torch, plot_title='Torch CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_results = pd.DataFrame(torch_cnn.log, index=[0])\n",
    "torch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(ytrain_breast))\n",
    "\n",
    "torch_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Conv2d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(in_features=64, out_features=n_classes, bias=True)\n",
    "    )\n",
    "\n",
    "summary(torch_model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "torch_cnn = TorchCNN(model=torch_model)\n",
    "\n",
    "# train model\n",
    "torch_cnn = torch_cnn.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_torch = torch_cnn.evaluate(xtest_breast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(ytest_breast, ypred_torch, plot_title='Torch CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_results = pd.DataFrame(torch_cnn.log, index=[0])\n",
    "torch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(ytrain_organ))\n",
    "\n",
    "torch_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Conv2d(in_channels=2, out_channels=4, kernel_size=5),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(in_features=64, out_features=n_classes, bias=True)\n",
    "    )\n",
    "\n",
    "summary(torch_model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "torch_cnn = TorchCNN(model=torch_model)\n",
    "\n",
    "# train model\n",
    "torch_cnn = torch_cnn.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    weight_decay=0.0001\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_torch = torch_cnn.evaluate(xtest_organ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(ytest_organ, ypred_torch, plot_title='Torch CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_results = pd.DataFrame(torch_cnn.log, index=[0])\n",
    "torch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteCNN():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'compile': None,\n",
    "            'keygen': None,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=30, learning_rate=0.0001, weight_decay=0.0001, plot=True):\n",
    "        # get number of classes\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        # data to tensor\n",
    "        x_train, y_train = torch.tensor(x_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long).flatten()\n",
    "        x_val, y_val = torch.tensor(x_val, dtype=torch.float), torch.tensor(y_val, dtype=torch.long).flatten()\n",
    "        # criterion\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # train loop\n",
    "        train_l, val_l, train_score, val_score= [], [], [], []\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            # set model to train mode\n",
    "            self.model.train()\n",
    "            start_time = time.time()\n",
    "            for i in range(0, len(x_train), 1):\n",
    "                inputs, labels = x_train[i:i+1], y_train[i:i+1]\n",
    "                # forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "            epoch_time = time.time() - start_time\n",
    "            self.log['train'] += epoch_time\n",
    "\n",
    "            # set model to eval mode\n",
    "            self.model.eval()\n",
    "            # calculate curve metrics\n",
    "            with torch.no_grad():\n",
    "                train_outputs = self.model(x_train)\n",
    "                train_loss = criterion(train_outputs, y_train)\n",
    "                predicted = torch.argmax(train_outputs, dim=1)\n",
    "                train_accuracy = accuracy_score(predicted.detach().numpy(), y_train.detach().numpy())*100\n",
    "\n",
    "                val_outputs = self.model(x_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                predicted = torch.argmax(val_outputs, dim=1)\n",
    "                val_accuracy = accuracy_score(predicted.detach().numpy(), y_val.detach().numpy())*100\n",
    "\n",
    "            train_l.append(train_loss.item())\n",
    "            val_l.append(val_loss.item())\n",
    "            train_score.append(train_accuracy)\n",
    "            val_score.append(val_accuracy)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].plot(train_l, label='Training Loss')\n",
    "            ax[0].plot(val_l, label='Validation Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].legend()\n",
    "            ax[1].plot(train_score, label='Training Accuracy')\n",
    "            ax[1].plot(val_score, label='Validation Accuracy')\n",
    "            ax[1]. set_xlabel('Epochs')\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].legend()\n",
    "            plt.show()\n",
    "\n",
    "        # compile model\n",
    "        start_time = time.time()\n",
    "        self.model = compile_brevitas_qat_model(self.model, x_train)\n",
    "        self.log['compile'] = time.time() - start_time\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X, fhe='simulate'):\n",
    "        # keygen\n",
    "        start_time = time.time()\n",
    "        self.model.fhe_circuit.keygen(force=True)\n",
    "        self.log['keygen'] = time.time() - start_time\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # predict each sample individually\n",
    "            outputs = [self.model.forward(X[[i]], fhe=fhe) for i in tqdm(range(X.shape[0]))]\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "        # get binary predictions\n",
    "        y_pred = torch.argmax(torch.cat(outputs), dim=1).detach().numpy().flatten()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    brevitas_model = torch.nn.Sequential(\n",
    "    # entry point of a network has to be a QuantIdentity layer\n",
    "    # to quantize the inputs\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # brevitas QuantConv2d layer\n",
    "    brevitas.nn.QuantConv2d(in_channels=1, out_channels=1, kernel_size=3, weight_bit_width=n_bits),\n",
    "    # brevitas QuantReLU layer\n",
    "    brevitas.nn.QuantReLU(bit_width=n_bits),\n",
    "    # torch AvgPool2d layer\n",
    "    torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # after a PyTorch layer there needs to follow a QuantIdentity layer\n",
    "    # to make sure the inputs are quantized correctly\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # PyTorch flatten layer\n",
    "    torch.nn.Flatten(),\n",
    "    # again a QuantIdentity layer after the PyTorch layer\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # brevitas QuantLinear (output) layer\n",
    "    brevitas.nn.QuantLinear(in_features=169, out_features=2, bias=True, weight_bit_width=n_bits)\n",
    "    )\n",
    "\n",
    "    concrete_cnn = ConcreteCNN(model=brevitas_model)\n",
    "    concrete_cnn = concrete_cnn.train(x_train=xtrain_pneumonia, y_train=ytrain_pneumonia, x_val=xval_pneumonia, y_val=yval_pneumonia, epochs=30, learning_rate=0.0001, weight_decay=0.0001, plot=False)\n",
    "    ypred_concrete = concrete_cnn.evaluate(xtest_pneumonia, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_pneumonia, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_pneumonia, ypred_concrete),\n",
    "        'recall':     recall_score(ytest_pneumonia, ypred_concrete),\n",
    "        'f1':         f1_score(ytest_pneumonia, ypred_concrete)\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_cnn.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_pneumonia, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    brevitas_model = torch.nn.Sequential(\n",
    "    # entry point of a network has to be a QuantIdentity layer\n",
    "    # to quantize the inputs\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # brevitas QuantConv2d layer\n",
    "    brevitas.nn.QuantConv2d(in_channels=1, out_channels=1, kernel_size=3, weight_bit_width=n_bits),\n",
    "    # brevitas QuantReLU layer\n",
    "    brevitas.nn.QuantReLU(bit_width=n_bits),\n",
    "    # torch AvgPool2d layer\n",
    "    torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # after a PyTorch layer there needs to follow a QuantIdentity layer\n",
    "    # to make sure the inputs are quantized correctly\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # PyTorch flatten layer\n",
    "    torch.nn.Flatten(),\n",
    "    # again a QuantIdentity layer after the PyTorch layer\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # brevitas QuantLinear (output) layer\n",
    "    brevitas.nn.QuantLinear(in_features=169, out_features=2, bias=True, weight_bit_width=n_bits)\n",
    "    )\n",
    "\n",
    "    concrete_cnn = ConcreteCNN(model=brevitas_model)\n",
    "    concrete_cnn = concrete_cnn.train(x_train=xtrain_breast, y_train=ytrain_breast, x_val=xval_breast, y_val=yval_breast, epochs=30, learning_rate=0.0001, weight_decay=0.0001, plot=False)\n",
    "    ypred_concrete = concrete_cnn.evaluate(xtest_breast, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_breast, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_breast, ypred_concrete),\n",
    "        'recall':     recall_score(ytest_breast, ypred_concrete),\n",
    "        'f1':         f1_score(ytest_breast, ypred_concrete)\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_cnn.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_breast, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "    \n",
    "    brevitas_model = torch.nn.Sequential(\n",
    "    # entry point of a network has to be a QuantIdentity layer\n",
    "    # to quantize the inputs\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # brevitas QuantConv2d layer\n",
    "    brevitas.nn.QuantConv2d(in_channels=1, out_channels=1, kernel_size=3, weight_bit_width=n_bits),\n",
    "    # brevitas QuantReLU layer\n",
    "    brevitas.nn.QuantReLU(bit_width=n_bits),\n",
    "    # torch AvgPool2d layer\n",
    "    torch.nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # after a PyTorch layer there needs to follow a QuantIdentity layer\n",
    "    # to make sure the inputs are quantized correctly\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # PyTorch flatten layer\n",
    "    torch.nn.Flatten(),\n",
    "    # again a QuantIdentity layer after the PyTorch layer\n",
    "    brevitas.nn.QuantIdentity(bit_width=n_bits, return_quant_tensor=True),\n",
    "    # brevitas QuantLinear (output) layer\n",
    "    brevitas.nn.QuantLinear(in_features=169, out_features=2, bias=True, weight_bit_width=n_bits)\n",
    "    )\n",
    "\n",
    "    concrete_cnn = ConcreteCNN(model=brevitas_model)\n",
    "    concrete_cnn = concrete_cnn.train(x_train=xtrain_organ, y_train=ytrain_organ, x_val=xval_organ, y_val=yval_organ, epochs=30, learning_rate=0.0001, weight_decay=0.0001, plot=False)\n",
    "    ypred_concrete = concrete_cnn.evaluate(xtest_organ, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_organ, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_organ, ypred_concrete, average='macro'),\n",
    "        'recall':     recall_score(ytest_organ, ypred_concrete, average='macro'),\n",
    "        'f1':         f1_score(ytest_organ, ypred_concrete, average='macro')\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_cnn.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_organ, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Pre-Trained Model & ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBPretrained():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.pretrained_model = None\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, n_estimators=100, max_depth=6, learning_rate=0.3):\n",
    "        # transform data to tensor\n",
    "        x_train, x_val = torch.tensor(x_train, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "        # combine data\n",
    "        X, y = torch.cat((x_train, x_val), dim=0), np.concatenate((y_train, y_val), axis=0)\n",
    "        # get resnet50 model\n",
    "        self.pretrained_model = resnet50(pretrained=True)\n",
    "        # modify first layer to accept grayscale images with one channel\n",
    "        self.pretrained_model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # remove last layer and replace with flattening layer\n",
    "        self.pretrained_model.fc = torch.nn.Flatten()\n",
    "        # feature extraction\n",
    "        with torch.no_grad():\n",
    "            X = self.pretrained_model(X).detach().numpy()\n",
    "        # initialize XGBoost model\n",
    "        self.model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, n_jobs=-1)\n",
    "        # train model\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X, y)\n",
    "        self.log['train'] = time.time() - start_time\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        # transform data to tensor\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        # feature extraction\n",
    "        with torch.no_grad():\n",
    "            X = self.pretrained_model(X).detach().numpy()\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "xgb_pretrained = XGBPretrained()\n",
    "\n",
    "# train model\n",
    "xgb_pretrained = xgb_pretrained.train(\n",
    "    x_train=xtrain_pneumonia,\n",
    "    y_train=ytrain_pneumonia,\n",
    "    x_val=xval_pneumonia,\n",
    "    y_val=yval_pneumonia,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.3\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_xgb_pretrained = xgb_pretrained.evaluate(xtest_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(ytest_pneumonia, ypred_xgb_pretrained, plot_title='XGB Pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = pd.DataFrame(xgb_pretrained.log, index=[0])\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "xgb_pretrained = XGBPretrained()\n",
    "\n",
    "# train model\n",
    "xgb_pretrained = xgb_pretrained.train(\n",
    "    x_train=xtrain_breast,\n",
    "    y_train=ytrain_breast,\n",
    "    x_val=xval_breast,\n",
    "    y_val=yval_breast,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.3\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_xgb_pretrained = xgb_pretrained.evaluate(xtest_breast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(ytest_breast, ypred_xgb_pretrained, plot_title='XGB Pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = pd.DataFrame(xgb_pretrained.log, index=[0])\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganCMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "xgb_pretrained = XGBPretrained()\n",
    "\n",
    "# train model\n",
    "xgb_pretrained = xgb_pretrained.train(\n",
    "    x_train=xtrain_organ,\n",
    "    y_train=ytrain_organ,\n",
    "    x_val=xval_organ,\n",
    "    y_val=yval_organ,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.3\n",
    "    )\n",
    "\n",
    "# evaluate model\n",
    "ypred_xgb_pretrained = xgb_pretrained.evaluate(xtest_organ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_metrics(ytest_organ, ypred_xgb_pretrained, plot_title='XGB Pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = pd.DataFrame(xgb_pretrained.log, index=[0])\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcretePretrained():\n",
    "\n",
    "    def __init__(self, n_bits=2):\n",
    "        self.n_bits = n_bits\n",
    "        self.model = None\n",
    "        self.pretrained_model = None\n",
    "        self.log = {\n",
    "            'train': 0,\n",
    "            'compile': None,\n",
    "            'keygen': None,\n",
    "            'evaluate_total': None,\n",
    "            'evaluate_sample': None\n",
    "        }\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, n_estimators=20, max_depth=3, learning_rate=0.1):\n",
    "        # transform data to tensor\n",
    "        x_train, x_val = torch.tensor(x_train, dtype=torch.float), torch.tensor(x_val, dtype=torch.float)\n",
    "        # combine data\n",
    "        X, y = torch.cat((x_train, x_val), dim=0), np.concatenate((y_train, y_val), axis=0)\n",
    "        # get resnet50 model\n",
    "        self.pretrained_model = resnet50(pretrained=True)\n",
    "        # modify first layer to accept grayscale images with one channel\n",
    "        self.pretrained_model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # remove last layer and replace with flattening layer\n",
    "        self.pretrained_model.fc = torch.nn.Flatten()\n",
    "        # feature extraction\n",
    "        with torch.no_grad():\n",
    "            X = self.pretrained_model(X).detach().numpy()\n",
    "        # initialize XGBoost model\n",
    "        self.model = ConcreteXGBClassifier(n_bits=self.n_bits, n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, n_jobs=-1)\n",
    "        # train model\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X, y)\n",
    "        self.log['train'] = time.time() - start_time\n",
    "        # compile model\n",
    "        start_time = time.time()\n",
    "        self.model.fhe_circuit = self.model.compile(X[:100])\n",
    "        self.log['compile'] = time.time() - start_time\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def evaluate(self, X, fhe='simulate'):\n",
    "        # transform data to tensor\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        # feature extraction\n",
    "        with torch.no_grad():\n",
    "            X = self.pretrained_model(X).detach().numpy()\n",
    "        # keygen\n",
    "        start_time = time.time()\n",
    "        self.model.fhe_circuit.keygen(force=True)\n",
    "        self.log['keygen'] = time.time() - start_time\n",
    "        # predict\n",
    "        start_time = time.time()\n",
    "        y_pred = np.array([self.model.predict(X[[i]], fhe=fhe)[0] for i in tqdm(range(X.shape[0]))])\n",
    "        self.log['evaluate_total'] = time.time() - start_time\n",
    "        self.log['evaluate_sample'] = self.log['evaluate_total']/X.shape[0]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PneumoniaMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    concrete_pretrained = ConcretePretrained(n_bits=n_bits)\n",
    "    concrete_pretrained = concrete_pretrained.train(x_train=xtrain_pneumonia, y_train=ytrain_pneumonia, x_val=xval_pneumonia, y_val=yval_pneumonia, n_estimators=20, max_depth=3, learning_rate=0.1)\n",
    "    ypred_concrete = concrete_pretrained.evaluate(xtest_pneumonia, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_pneumonia, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_pneumonia, ypred_concrete),\n",
    "        'recall':     recall_score(ytest_pneumonia, ypred_concrete),\n",
    "        'f1':         f1_score(ytest_pneumonia, ypred_concrete)\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_pretrained.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_pneumonia, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    concrete_pretrained = ConcretePretrained(n_bits=n_bits)\n",
    "    concrete_pretrained = concrete_pretrained.train(x_train=xtrain_breast, y_train=ytrain_breast, x_val=xval_breast, y_val=yval_breast, n_estimators=20, max_depth=3, learning_rate=0.1)\n",
    "    ypred_concrete = concrete_pretrained.evaluate(xtest_breast, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_breast, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_breast, ypred_concrete),\n",
    "        'recall':     recall_score(ytest_breast, ypred_concrete),\n",
    "        'f1':         f1_score(ytest_breast, ypred_concrete)\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_pretrained.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_breast, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OrganMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'n_bits': [], 'y_pred': [], 'times': [], 'metrics': []}\n",
    "\n",
    "for n_bits in range(2,7):\n",
    "\n",
    "    concrete_pretrained = ConcretePretrained(n_bits=n_bits)\n",
    "    concrete_pretrained = concrete_pretrained.train(x_train=xtrain_organ, y_train=ytrain_organ, x_val=xval_organ, y_val=yval_organ, n_estimators=20, max_depth=3, learning_rate=0.1)\n",
    "    ypred_concrete = concrete_pretrained.evaluate(xtest_organ, fhe=mode)\n",
    "    metrics = {\n",
    "        'accuracy':   accuracy_score(ytest_organ, ypred_concrete),\n",
    "        'precision':  precision_score(ytest_organ, ypred_concrete, average='macro'),\n",
    "        'recall':     recall_score(ytest_organ, ypred_concrete, average='macro'),\n",
    "        'f1':         f1_score(ytest_organ, ypred_concrete, average='macro')\n",
    "    }\n",
    "\n",
    "    results['n_bits'].append(n_bits)\n",
    "    results['y_pred'].append(ypred_concrete)\n",
    "    results['times'].append(concrete_pretrained.log)\n",
    "    results['metrics'].append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for preds, bits in zip(results['y_pred'], results['n_bits']):\n",
    "    print('#################################################################')\n",
    "    plot_classification_metrics(ytest_organ, preds, plot_title=f'Quantization (n_bits={bits})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_results = pd.concat([pd.DataFrame.from_records(results['times']), pd.DataFrame.from_records(results['metrics'])], axis=1)\n",
    "concrete_results.insert(0, 'n_bits', results['n_bits'])\n",
    "concrete_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
